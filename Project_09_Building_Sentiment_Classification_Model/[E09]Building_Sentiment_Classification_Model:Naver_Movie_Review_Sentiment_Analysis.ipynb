{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saerannn/ML-DL-Project/blob/main/Project_09_Building_Sentiment_Classification_Model/%5BE09%5DBuilding_Sentiment_Classification_Model%3ANaver_Movie_Review_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926261ad",
      "metadata": {
        "id": "926261ad",
        "outputId": "010da28c-c16e-4cff-f308-345c2c9bd62a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.3.3\n",
            "0.5.2\n",
            "4.1.2\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "import konlpy\n",
        "import gensim\n",
        "\n",
        "print(pandas.__version__)\n",
        "print(konlpy.__version__)\n",
        "print(gensim.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e544e4d6",
      "metadata": {
        "id": "e544e4d6"
      },
      "source": [
        "# **[Project_09_Building Sentiment Classification Model_ Naver Movie Review Sentiment Analysis]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94df8eba",
      "metadata": {
        "id": "94df8eba"
      },
      "source": [
        "## **1. 데이터 불러오기 및 확인**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a76dfb4a",
      "metadata": {
        "id": "a76dfb4a"
      },
      "source": [
        "### **1-1. 라이브러리 불러오기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662d8b52",
      "metadata": {
        "id": "662d8b52"
      },
      "outputs": [],
      "source": [
        "############################################## \b라이브러리 불러오기 #####################################################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.3f}'.format #지수표현식말고 실수로 표현\n",
        "import platform\n",
        "import random\n",
        "import statsmodels\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(action = \"ignore\")\n",
        "import missingno as msno\n",
        "import scipy as sp\n",
        "from scipy import stats\n",
        "from scipy.stats import norm\n",
        "import re\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "# 시각화\n",
        "import seaborn as sns\n",
        "sns.set(font_scale=1.5)\n",
        "import matplotlib as mpl  # 기본 설정 만지는 용도\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina' \n",
        "plt.rcParams['axes.unicode_minus'] = False \n",
        "plt.style.use('seaborn')\n",
        "\n",
        "\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from konlpy.tag import Mecab\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "# 모델\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf676010",
      "metadata": {
        "id": "bf676010"
      },
      "source": [
        "### **1-2. 데이터 불러오기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "277d4602",
      "metadata": {
        "id": "277d4602"
      },
      "outputs": [],
      "source": [
        "############################################## 데이터 불러오기 #####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
        "\n",
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab7011b",
      "metadata": {
        "id": "8ab7011b",
        "outputId": "fd949064-2a95-4a68-c1b4-2eff73c7266f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data shape:  (150000, 3)\n",
            "         id                                           document  label\n",
            "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
            "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
            "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
            "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
            "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"train_data shape: \", train_data.shape)\n",
        "\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba04345",
      "metadata": {
        "id": "0ba04345",
        "outputId": "6e3413e0-d4f1-4cae-b9bf-70503b736f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_data shape:  (50000, 3)\n",
            "        id                                           document  label\n",
            "0  6270596                                                굳 ㅋ      1\n",
            "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
            "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
            "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
            "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"test_data shape: \", test_data.shape)\n",
        "\n",
        "print(test_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b431ef",
      "metadata": {
        "id": "32b431ef"
      },
      "source": [
        "## **2. 데이터 탐색 및 전처리**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34defb79",
      "metadata": {
        "id": "34defb79"
      },
      "source": [
        "### **2-1. 데이터 로더 구성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb351e7",
      "metadata": {
        "id": "cbb351e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "tokenizer = Mecab()\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "\n",
        "def load_data(train_data, test_data, num_words=10000):\n",
        "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "    train_data = train_data.dropna(how = 'any') \n",
        "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "    test_data = test_data.dropna(how = 'any') \n",
        "    \n",
        "    X_train = []\n",
        "    for sentence in train_data['document']:\n",
        "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
        "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "        X_train.append(temp_X)\n",
        "\n",
        "    X_test = []\n",
        "    for sentence in test_data['document']:\n",
        "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
        "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "        X_test.append(temp_X)\n",
        "    \n",
        "    words = np.concatenate(X_train).tolist()\n",
        "    counter = Counter(words)\n",
        "    counter = counter.most_common(10000-4)\n",
        "    vocab = ['', '', '', ''] + [key for key, _ in counter]\n",
        "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
        "        \n",
        "    def wordlist_to_indexlist(wordlist):\n",
        "        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
        "        \n",
        "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
        "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
        "        \n",
        "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
        "    \n",
        "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb616bd0",
      "metadata": {
        "id": "cb616bd0"
      },
      "outputs": [],
      "source": [
        "index_to_word = {index:word for word, index in word_to_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d5be96",
      "metadata": {
        "id": "92d5be96"
      },
      "outputs": [],
      "source": [
        "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
        "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
        "def get_encoded_sentence(sentence, word_to_index):\n",
        "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
        "\n",
        "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
        "def get_encoded_sentences(sentences, word_to_index):\n",
        "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
        "\n",
        "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
        "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
        "\n",
        "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
        "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
        "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25457704",
      "metadata": {
        "id": "25457704"
      },
      "source": [
        "### **2-2. 데이터셋 내 문장 길이 분포 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0279eb8",
      "metadata": {
        "id": "a0279eb8"
      },
      "outputs": [],
      "source": [
        "total_data_text = list(x_train) + list(x_test)\n",
        "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
        "num_tokens = [len(tokens) for tokens in total_data_text]\n",
        "num_tokens = np.array(num_tokens)\n",
        "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
        "print('문장길이 평균 : ', np.mean(num_tokens))\n",
        "print('문장길이 최대 : ', np.max(num_tokens))\n",
        "print('문장길이 표준편차 : ', np.std(num_tokens))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e56b48",
      "metadata": {
        "id": "50e56b48"
      },
      "source": [
        "### **2-3. 적절한 최대 문장 길이 지정**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1db7ae",
      "metadata": {
        "id": "ba1db7ae"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "maxlen = int(max_tokens)\n",
        "print('pad_sequences maxlen : ', maxlen)\n",
        "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd28016",
      "metadata": {
        "id": "0cd28016"
      },
      "source": [
        "### **2-4. Padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312ddc33",
      "metadata": {
        "id": "312ddc33"
      },
      "outputs": [],
      "source": [
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        value=word_to_index[\"<PAD>\"],\n",
        "                                                        padding='post', # 혹은 'pre'\n",
        "                                                        maxlen=maxlen)\n",
        "\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                       value=word_to_index[\"<PAD>\"],\n",
        "                                                       padding='post', # 혹은 'pre'\n",
        "                                                       maxlen=maxlen)\n",
        "\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c848294",
      "metadata": {
        "id": "3c848294"
      },
      "source": [
        "## **3. 모델 구현**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adc93c6e",
      "metadata": {
        "id": "adc93c6e"
      },
      "source": [
        "## **3-1. validation set 구성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08a2da8",
      "metadata": {
        "id": "c08a2da8"
      },
      "outputs": [],
      "source": [
        "# validation set 10000건 분리\n",
        "x_val = x_train[:10000]   \n",
        "y_val = y_train[:10000]\n",
        "\n",
        "# validation set을 제외한 나머지 15000건\n",
        "partial_x_train = x_train[10000:]  \n",
        "partial_y_train = y_train[10000:]\n",
        "\n",
        "print(partial_x_train.shape)\n",
        "print(partial_y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12df6fa4",
      "metadata": {
        "id": "12df6fa4"
      },
      "source": [
        "## **3-2. 모델 구성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c318fa65",
      "metadata": {
        "id": "c318fa65"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f92712",
      "metadata": {
        "id": "29f92712"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56adef8f",
      "metadata": {
        "id": "56adef8f",
        "outputId": "ee23c172-1cdd-4bcf-9037-07477202f446"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_51/2310098207.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "#################### 1) LSTM 모델 구성 #######################\n",
        "\n",
        "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
        "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
        "\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1263bbf",
      "metadata": {
        "id": "a1263bbf"
      },
      "outputs": [],
      "source": [
        "#################### 2) 1-D Convolution Neural Network(1-D CNN) 모델 구성 #######################\n",
        "\n",
        "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
        "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
        "\n",
        "\n",
        "# model 설계 \n",
        "model = tf.keras.Sequential()\n",
        "# [[YOUR CODE]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
        "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23ea5d0",
      "metadata": {
        "id": "e23ea5d0",
        "outputId": "83285197-6734-40ca-e95d-f14b72e114d9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_51/1712498335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# [[YOUR CODE]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "#################### 3) GlobalMaxPooling1D 모델 구성 #######################\n",
        "\n",
        "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
        "model = tf.keras.Sequential()\n",
        "# [[YOUR CODE]]\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
        "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f8c030c",
      "metadata": {
        "id": "4f8c030c"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dcb7917",
      "metadata": {
        "id": "0dcb7917"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4c4025",
      "metadata": {
        "id": "4d4c4025"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a73ff030",
      "metadata": {
        "id": "a73ff030"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4945aad",
      "metadata": {
        "id": "d4945aad"
      },
      "source": [
        "## **3-3. 모델 훈련 개시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5503090e",
      "metadata": {
        "id": "5503090e"
      },
      "outputs": [],
      "source": [
        "# 모델 학습\n",
        "\n",
        "src_path =  os.path.join(os.getcwd(),\"src\")\n",
        "src_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "              \n",
        "# epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
        "\n",
        "# history = model.fit(partial_x_train,\n",
        "#                     partial_y_train,\n",
        "#                     epochs=epochs,\n",
        "#                     batch_size=512,\n",
        "#                     validation_data=(x_val, y_val),\n",
        "#                     verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9459626",
      "metadata": {
        "id": "a9459626"
      },
      "outputs": [],
      "source": [
        "# 모델 평가\n",
        "results = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9405f899",
      "metadata": {
        "id": "9405f899"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b9e0650",
      "metadata": {
        "id": "5b9e0650"
      },
      "source": [
        "## **3-4. Loss, Accuracy 그래프 시각화**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8228e3ae",
      "metadata": {
        "id": "8228e3ae"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4adc7dbc",
      "metadata": {
        "id": "4adc7dbc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979e26c6",
      "metadata": {
        "id": "979e26c6"
      },
      "source": [
        "## **3-5. 학습된 Embedding 레이어 분석**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfe9fb8",
      "metadata": {
        "id": "5dfe9fb8"
      },
      "outputs": [],
      "source": [
        "embedding_layer = model.layers[0]\n",
        "weights = embedding_layer.get_weights()[0]\n",
        "print(weights.shape)    # shape: (vocab_size, embedding_dim)\n",
        "\n",
        "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
        "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
        "f = open(word2vec_file_path, 'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
        "\n",
        "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
        "vectors = model.get_weights()[0]\n",
        "for i in range(4,vocab_size):\n",
        "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
        "f.close()\n",
        "\n",
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "\n",
        "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
        "vector = word_vectors['computer']\n",
        "vector\n",
        "\n",
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "\n",
        "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
        "vector = word_vectors['computer']\n",
        "vector\n",
        "\n",
        "word_vectors.similar_by_word(\"love\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c12e4c",
      "metadata": {
        "id": "94c12e4c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa145cc",
      "metadata": {
        "id": "0fa145cc"
      },
      "source": [
        "## **3-6. 한국어 Word2Vec 임베딩 활용하여 성능 개선**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac8751d",
      "metadata": {
        "id": "dac8751d"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
        "vector = word2vec['computer']\n",
        "vector     # 무려 300dim의 워드 벡터입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c91ce4a",
      "metadata": {
        "id": "9c91ce4a"
      },
      "outputs": [],
      "source": [
        "# 메모리를 다소 많이 소비하는 작업이니 유의해 주세요.\n",
        "word2vec.similar_by_word(\"love\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5faa238f",
      "metadata": {
        "id": "5faa238f"
      },
      "outputs": [],
      "source": [
        "embedding_layer = model.layers[0]\n",
        "weights = embedding_layer.get_weights()[0]\n",
        "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "070dcb65",
      "metadata": {
        "id": "070dcb65"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b8f0ab",
      "metadata": {
        "id": "05b8f0ab"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f166c34",
      "metadata": {
        "id": "0f166c34"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6a6ea6",
      "metadata": {
        "id": "4f6a6ea6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\"는 \"파란색 점\"입니다\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b는 \"파란 실선\"입니다\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e909af30",
      "metadata": {
        "id": "e909af30"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # 그림을 초기화합니다\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe328e6",
      "metadata": {
        "id": "6fe328e6"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
        "word_vector_dim = 300  # 워드 벡터의 차원수\n",
        "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
        "\n",
        "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
        "for i in range(4,vocab_size):\n",
        "    if index_to_word[i] in word2vec:\n",
        "        embedding_matrix[i] = word2vec[index_to_word[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab827eb8",
      "metadata": {
        "id": "ab827eb8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
        "word_vector_dim = 300  # 워드 벡터의 차원 수 \n",
        "\n",
        "# 모델 구성\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, \n",
        "                                 word_vector_dim, \n",
        "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
        "                                 input_length=maxlen, \n",
        "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
        "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387b7f8d",
      "metadata": {
        "id": "387b7f8d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72497a4d",
      "metadata": {
        "id": "72497a4d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84883e8d",
      "metadata": {
        "id": "84883e8d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a75a93d",
      "metadata": {
        "id": "7a75a93d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a549621",
      "metadata": {
        "id": "1a549621"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f85ed3",
      "metadata": {
        "id": "01f85ed3"
      },
      "outputs": [],
      "source": [
        "# 학습의 진행\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be99ab6",
      "metadata": {
        "id": "6be99ab6"
      },
      "outputs": [],
      "source": [
        "# 테스트셋을 통한 모델 평가\n",
        "results = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5977b5",
      "metadata": {
        "id": "de5977b5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8caf8912",
      "metadata": {
        "id": "8caf8912"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "[E09]Building Sentiment Classification Model_ Naver Movie Review Sentiment Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}